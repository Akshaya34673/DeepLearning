{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2dc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69300ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Test data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5585294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.Resizing(224, 224),\n",
    "    layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.map(lambda x, y: (resize_and_rescale(x), y))\n",
    "train_ds = train_ds.shuffle(buffer_size=10000).batch(64)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.map(lambda x, y: (resize_and_rescale(x), y))\n",
    "test_ds = test_ds.batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)         11712     \n",
      " batch_normalization (BatchN (None, 54, 54, 96)         384       \n",
      " max_pooling2d (MaxPooling2D (None, 26, 26, 96)         0         \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 256)        614656    \n",
      " batch_normalization_1 (Batc (None, 26, 26, 256)        1024      \n",
      " max_pooling2d_1 (MaxPooling (None, 12, 12, 256)        0         \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 384)        885120    \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 384)        1327488   \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)        884992    \n",
      " max_pooling2d_2 (MaxPooling (None, 5, 5, 256)          0         \n",
      " flatten (Flatten)           (None, 6400)               0         \n",
      " dense (Dense)               (None, 4096)               26218496  \n",
      " dropout (Dropout)           (None, 4096)               0         \n",
      " dense_1 (Dense)             (None, 4096)               16781312  \n",
      " dropout_1 (Dropout)         (None, 4096)               0         \n",
      " dense_2 (Dense)             (None, 10)                 40970     \n",
      "=================================================================\n",
      "Total params: 46,072,154\n",
      "Trainable params: 46,071,514\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build modified AlexNet\n",
    "def build_alexnet():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(96, (11,11), strides=4, activation='relu', input_shape=(224,224,1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Conv2D(256, (5,5), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(384, (3,3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(256, (3,3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(3,3), strides=2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4096, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_alexnet()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 45s 48ms/step - loss: 1.3842 - accuracy: 0.5258 - val_loss: 0.8253 - val_accuracy: 0.7135\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.6812 - accuracy: 0.7608 - val_loss: 0.6247 - val_accuracy: 0.7787\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.5476 - accuracy: 0.8099 - val_loss: 0.5523 - val_accuracy: 0.8056\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.4688 - accuracy: 0.8372 - val_loss: 0.4975 - val_accuracy: 0.8283\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.4122 - accuracy: 0.8566 - val_loss: 0.4819 - val_accuracy: 0.8356\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.3678 - accuracy: 0.8712 - val_loss: 0.4548 - val_accuracy: 0.8415\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.3306 - accuracy: 0.8831 - val_loss: 0.4403 - val_accuracy: 0.8492\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.2978 - accuracy: 0.8952 - val_loss: 0.4329 - val_accuracy: 0.8526\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.2699 - accuracy: 0.9055 - val_loss: 0.4257 - val_accuracy: 0.8558\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.2435 - accuracy: 0.9141 - val_loss: 0.4125 - val_accuracy: 0.8604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
